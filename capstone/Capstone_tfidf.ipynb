{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.6 (default, Oct 26 2016, 20:30:19) \n",
      "[GCC 4.8.4]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from random import shuffle\n",
    "import sys\n",
    "\n",
    "print sys.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  Class\n",
       "0   0  FAM58A  Truncating Mutations      1\n",
       "1   1     CBL                 W802*      2\n",
       "2   2     CBL                 Q249E      2\n",
       "3   3     CBL                 N454D      3\n",
       "4   4     CBL                 L399V      4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_variant_data = pd.read_csv('training_variants')\n",
    "X_train_variant_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321\n"
     ]
    }
   ],
   "source": [
    "print X_train_variant_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ACSL4</td>\n",
       "      <td>R570S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NAGLU</td>\n",
       "      <td>P521L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PAH</td>\n",
       "      <td>L333F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ING1</td>\n",
       "      <td>A148D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>TMEM216</td>\n",
       "      <td>G77A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Gene Variation\n",
       "0   0    ACSL4     R570S\n",
       "1   1    NAGLU     P521L\n",
       "2   2      PAH     L333F\n",
       "3   3     ING1     A148D\n",
       "4   4  TMEM216      G77A"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_variant_data = pd.read_csv('test_variants')\n",
    "X_test_variant_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2. This mutation resulted in a myeloproliferat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Abstract The Large Tumor Suppressor 1 (LATS1)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Vascular endothelial growth factor receptor (V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Inflammatory myofibroblastic tumor (IMT) is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Abstract Retinoblastoma is a pediatric retina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Text\n",
       "0   0  2. This mutation resulted in a myeloproliferat...\n",
       "1   1   Abstract The Large Tumor Suppressor 1 (LATS1)...\n",
       "2   2  Vascular endothelial growth factor receptor (V...\n",
       "3   3  Inflammatory myofibroblastic tumor (IMT) is a ...\n",
       "4   4   Abstract Retinoblastoma is a pediatric retina..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text = pd.read_table('training_text', sep='\\|\\|', engine='python', names=['ID', 'Text'], skiprows=[0])\n",
    "X_test_text = pd.read_table('test_text', sep='\\|\\|', engine='python', names=['ID', 'Text'], skiprows=[0])\n",
    "X_test_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2996\n"
     ]
    }
   ],
   "source": [
    "print len(np.unique(X_train_variant_data['Variation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    2\n",
       "3    3\n",
       "4    4\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Remove the class target label from the training variant data set and add it to an independent target label array\n",
    "\"\"\"\n",
    "y_train = X_train_variant_data['Class']\n",
    "X_train_variant_data.drop('Class', axis=1, inplace=True)\n",
    "y_train.head()\n",
    "#print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>class4</th>\n",
       "      <th>class5</th>\n",
       "      <th>class6</th>\n",
       "      <th>class7</th>\n",
       "      <th>class8</th>\n",
       "      <th>class9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class1  class2  class3  class4  class5  class6  class7  class8  class9\n",
       "0       1       0       0       0       0       0       0       0       0\n",
       "1       0       1       0       0       0       0       0       0       0\n",
       "2       0       1       0       0       0       0       0       0       0\n",
       "3       0       0       1       0       0       0       0       0       0\n",
       "4       0       0       0       1       0       0       0       0       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    One hot encode the target label array\n",
    "\"\"\"\n",
    "\n",
    "y_train = pd.get_dummies(y_train, prefix='class', prefix_sep='')\n",
    "y_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 500)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Hash vectorize gene, variation columns\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "\n",
    "X_train_gene = X_train_variant_data['Gene']\n",
    "gene_hash_vectorizer = HashingVectorizer(n_features=500)\n",
    "gene_vector = gene_hash_vectorizer.transform(X_train_gene)\n",
    "\n",
    "X_train_variation = X_train_variant_data['Variation']\n",
    "variation_hash_vectorizer = HashingVectorizer(n_features=5000)\n",
    "variation_vector = variation_hash_vectorizer.transform(X_train_variation)\n",
    "\n",
    "X_test_gene = X_test_variant_data['Gene']\n",
    "gene_test_hash_vectorizer = HashingVectorizer(n_features=500)\n",
    "gene_test_vector = gene_test_hash_vectorizer.transform(X_test_gene)\n",
    "\n",
    "X_test_variation = X_test_variant_data['Variation']\n",
    "variation_test_hash_vectorizer = HashingVectorizer(n_features=5000)\n",
    "variation_test_vector = variation_test_hash_vectorizer.transform(X_test_variation)\n",
    "\n",
    "print gene_vector.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Download nltk stopwords corpus\n",
    "\"\"\"\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i opinion guarante life\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Process clinical text data by tokenizing words, removing stop words, stemming words etc.\n",
    "\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import utils\n",
    "def process_clinical_text(clinical_text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenized_text = word_tokenize(utils.to_unicode(clinical_text))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    processed_text = []\n",
    "    for word in tokenized_text:\n",
    "        if word not in stop_words:\n",
    "            processed_text.append(stemmer.stem(word))\n",
    "    return \" \".join(processed_text)\n",
    "\n",
    "print process_clinical_text('I am of the opinion that there is no guarantee any where in life')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00808519  0.         ...,  0.          0.          0.        ]\n",
      "(3321, 145030)\n",
      "(5668, 145030)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Apply Tfidf vectorizer model on the clinical text training and test data\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vector = TfidfVectorizer()\n",
    "processed_clinical_text = X_train_text['Text'].apply(process_clinical_text)\n",
    "#print processed_clinical_text[0]\n",
    "tfidf_vector.fit(processed_clinical_text)\n",
    "clinical_text_train = tfidf_vector_train.transform(processed_clinical_text).toarray()\n",
    "#print clinical_text_train[0]\n",
    "processed_clinical_test_text = X_test_text['Text'].apply(process_clinical_text)\n",
    "clinical_text_test = tfidf_vector.transform(processed_clinical_test_text).toarray()\n",
    "print clinical_text_test[0]\n",
    "print clinical_text_train.shape\n",
    "print clinical_text_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 145030)\n"
     ]
    }
   ],
   "source": [
    "print clinical_text_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 150530)\n",
      "(3321, 9)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Combine gene vector, variation vector and clinical text vector into a resultant training set\n",
    "\"\"\"\n",
    "\n",
    "X_train = np.hstack((gene_vector.toarray(), variation_vector.toarray(), clinical_text_train))\n",
    "X_test = np.hstack((gene_test_vector.toarray(), variation_test_vector.toarray(), clinical_text_test))\n",
    "\n",
    "print X_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 150530, 1)\n",
      "(3321, 9)\n",
      "(5668, 150530, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Expand dimensions of data to fit into the CNN model\n",
    "\"\"\"\n",
    "\n",
    "X_train_dim_data = np.expand_dims(X_train, axis=2)\n",
    "print X_train_dim_data.shape\n",
    "y_train_dim_data = np.array(y_train)\n",
    "print y_train_dim_data.shape\n",
    "\n",
    "X_test_dim_data = np.expand_dims(X_test, axis=2)\n",
    "print X_test_dim_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Define and compile the CNN model\n",
    "\"\"\"\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "def create_cnn_model(optimizer='rmsprop'):\n",
    "    num_classes = 9\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=X_train_dim_data.shape[1:]))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_model.add(Dropout(0.2))\n",
    "    cnn_model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_model.add(Dropout(0.2))\n",
    "    cnn_model.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_model.add(Dropout(0.2))\n",
    "    cnn_model.add(GlobalAveragePooling1D())\n",
    "    cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    cnn_model.summary()\n",
    "    cnn_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 9)\n",
      "[1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Reshape the training inputs for KFold cross validation inputs\n",
    "\"\"\"\n",
    "X_train_2d = X_train_dim_data.reshape(X_train_dim_data.shape[0], X_train_dim_data.shape[1])\n",
    "y_train_2d = y_train_dim_data.reshape(y_train_dim_data.shape[0], y_train_dim_data.shape[1])\n",
    "print y_train_2d.shape\n",
    "print y_train_2d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 150530, 16)        48        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 75265, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 75265, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 75265, 32)         1056      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 37632, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 37632, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 37632, 64)         4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 18816, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18816, 64)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 5,849\n",
      "Trainable params: 5,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2988 samples, validate on 333 samples\n",
      "Epoch 1/3\n",
      "2980/2988 [============================>.] - ETA: 0s - loss: 1.8963 - acc: 0.2785Epoch 00000: val_loss improved from inf to 1.86151, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2988/2988 [==============================] - 67s - loss: 1.8962 - acc: 0.2795 - val_loss: 1.8615 - val_acc: 0.3303\n",
      "Epoch 2/3\n",
      "2980/2988 [============================>.] - ETA: 0s - loss: 1.8351 - acc: 0.2822Epoch 00001: val_loss did not improve\n",
      "2988/2988 [==============================] - 66s - loss: 1.8351 - acc: 0.2821 - val_loss: 1.8847 - val_acc: 0.3303\n",
      "Epoch 3/3\n",
      "2980/2988 [============================>.] - ETA: 0s - loss: 1.8330 - acc: 0.2822Epoch 00002: val_loss did not improve\n",
      "2988/2988 [==============================] - 66s - loss: 1.8330 - acc: 0.2821 - val_loss: 1.8925 - val_acc: 0.3303\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8384 - acc: 0.3158Epoch 00000: val_loss improved from 1.86151 to 1.86144, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 66s - loss: 1.8379 - acc: 0.3158 - val_loss: 1.8614 - val_acc: 0.0271\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8383 - acc: 0.3164Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8383 - acc: 0.3158 - val_loss: 1.9316 - val_acc: 0.0271\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8351 - acc: 0.3168Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8362 - acc: 0.3158 - val_loss: 1.9190 - val_acc: 0.0271\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8425 - acc: 0.2799Epoch 00000: val_loss improved from 1.86144 to 1.80108, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 66s - loss: 1.8424 - acc: 0.2797 - val_loss: 1.8011 - val_acc: 0.3524\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8437 - acc: 0.2792Epoch 00001: val_loss improved from 1.80108 to 1.78204, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 66s - loss: 1.8427 - acc: 0.2797 - val_loss: 1.7820 - val_acc: 0.3524\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8412 - acc: 0.2799Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8412 - acc: 0.2797 - val_loss: 1.8034 - val_acc: 0.3524\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8383 - acc: 0.2896Epoch 00000: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8383 - acc: 0.2897 - val_loss: 1.8300 - val_acc: 0.2620\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8360 - acc: 0.2899Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8359 - acc: 0.2897 - val_loss: 1.8211 - val_acc: 0.2620\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8369 - acc: 0.2906Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8379 - acc: 0.2897 - val_loss: 1.8265 - val_acc: 0.2620\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8417 - acc: 0.2691Epoch 00000: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8415 - acc: 0.2693 - val_loss: 1.8042 - val_acc: 0.4458\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8431 - acc: 0.2664Epoch 00001: val_loss improved from 1.78204 to 1.78067, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 66s - loss: 1.8428 - acc: 0.2666 - val_loss: 1.7807 - val_acc: 0.4458\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8432 - acc: 0.2698Epoch 00002: val_loss improved from 1.78067 to 1.77860, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 66s - loss: 1.8426 - acc: 0.2693 - val_loss: 1.7786 - val_acc: 0.4458\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8338 - acc: 0.2789Epoch 00000: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8336 - acc: 0.2790 - val_loss: 1.8607 - val_acc: 0.3584\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8326 - acc: 0.2792Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8334 - acc: 0.2790 - val_loss: 1.8616 - val_acc: 0.3584\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8339 - acc: 0.2789Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8335 - acc: 0.2790 - val_loss: 1.8627 - val_acc: 0.3584\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8450 - acc: 0.2926Epoch 00000: val_loss improved from 1.77860 to 1.76718, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 66s - loss: 1.8457 - acc: 0.2921 - val_loss: 1.7672 - val_acc: 0.2410\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8446 - acc: 0.2926Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8448 - acc: 0.2921 - val_loss: 1.7943 - val_acc: 0.2410\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8443 - acc: 0.2919Epoch 00002: val_loss improved from 1.76718 to 1.76319, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 66s - loss: 1.8441 - acc: 0.2921 - val_loss: 1.7632 - val_acc: 0.2410\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8001 - acc: 0.3081Epoch 00000: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8002 - acc: 0.3081 - val_loss: 2.2107 - val_acc: 0.0964\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8009 - acc: 0.3074Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8004 - acc: 0.3081 - val_loss: 2.1918 - val_acc: 0.0964\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.7994 - acc: 0.3084Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8000 - acc: 0.3081 - val_loss: 2.1934 - val_acc: 0.0964\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8159 - acc: 0.2862Epoch 00000: val_loss did not improve\n",
      "2989/2989 [==============================] - 65s - loss: 1.8157 - acc: 0.2860 - val_loss: 2.0499 - val_acc: 0.2952\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8140 - acc: 0.2866Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8145 - acc: 0.2860 - val_loss: 2.0379 - val_acc: 0.2952\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8121 - acc: 0.2866Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 66s - loss: 1.8134 - acc: 0.2860 - val_loss: 2.0313 - val_acc: 0.2952\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8494 - acc: 0.2681Epoch 00000: val_loss improved from 1.76319 to 1.73441, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 66s - loss: 1.8509 - acc: 0.2676 - val_loss: 1.7344 - val_acc: 0.4608\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8507 - acc: 0.2681Epoch 00001: val_loss improved from 1.73441 to 1.73233, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 66s - loss: 1.8501 - acc: 0.2676 - val_loss: 1.7323 - val_acc: 0.4608\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8493 - acc: 0.2664Epoch 00002: val_loss improved from 1.73233 to 1.71442, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 66s - loss: 1.8485 - acc: 0.2676 - val_loss: 1.7144 - val_acc: 0.4608\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Train the CNN model\n",
    "\"\"\"\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "cv_fold = KFold(n_splits=10)\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "for train_index, test_index in cv_fold.split(X_train_2d):\n",
    "    X_fold_train, X_fold_test = X_train_2d[train_index], X_train_2d[test_index]\n",
    "    y_fold_train, y_fold_test = y_train_2d[train_index], y_train_2d[test_index]\n",
    "    X_fold_train = np.expand_dims(X_fold_train, axis=2)\n",
    "    X_fold_test = np.expand_dims(X_fold_test, axis=2)    \n",
    "    cnn_model.fit(X_fold_train, y_fold_train, validation_data=(X_fold_test, y_fold_test), epochs=3, batch_size=20, \n",
    "              callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Load the best weights into the model\n",
    "\"\"\"\n",
    "cnn_model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17879988  0.12561478  0.02768872  0.21854559  0.07626835  0.08522249\n",
      "  0.270529    0.00536043  0.01197069]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Make predictions from test data\n",
    "\"\"\"\n",
    "\n",
    "#X_test_data = X_test_dim_data.reshape(X_test_dim_data.shape[1:])\n",
    "#print X_test_data.shape\n",
    "model_predictions = cnn_model.predict(X_test_dim_data)\n",
    "print model_predictions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(len(model_predictions)):\n",
    "    max_value = model_predictions[i][np.argmax(model_predictions[i])]\n",
    "    y_pred.append(model_predictions[i] // max_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print y_pred[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[1, 2, 2, 4, 4, 4, 9, 7, 7, 7, 2, 1, 4, 1, 1, 1, 2, 3, 2, 7, 2, 1, 7, 7, 3, 7, 9, 7, 1, 4, 1, 6, 4, 9, 6, 4, 1, 5, 2, 1, 3, 2, 7, 7, 5, 7, 4, 6, 4, 1, 1, 4, 7, 1, 6, 7, 1, 1, 7, 2, 7, 2, 1, 7, 4, 9, 7, 4, 4, 1, 4, 5, 1, 1, 1, 4, 2, 2, 1, 7, 1, 5, 6, 7, 6, 6, 1, 7, 5, 2, 1, 4, 7, 7, 4, 1, 1, 4, 7, 1, 1, 2, 3, 7, 2, 2, 7, 4, 1, 4, 4, 7, 4, 7, 7, 7, 6, 3, 7, 7, 4, 4, 4, 2, 4, 1, 4, 1, 4, 1, 2, 4, 7, 7, 4, 7, 7, 7, 4, 1, 2, 7, 4, 1, 1, 7, 2, 7, 7, 7, 2, 2, 1, 4, 1, 1, 7, 7, 1, 5, 4, 4, 7, 2, 1, 2, 4, 6, 4, 7, 1, 1, 1, 1, 7, 7, 4, 5, 4, 1, 4, 2, 6, 7, 4, 5, 7, 3, 1, 7, 6, 2, 1, 8, 7, 7, 5, 7, 5, 5, 1, 1, 1, 7, 7, 2, 1, 1, 7, 1, 2, 1, 4, 9, 1, 2, 7, 7, 1, 2, 4, 1, 4, 7, 7, 1, 4, 8, 4, 5, 4, 4, 5, 1, 4, 1, 2, 7, 2, 4, 1, 6, 6, 5, 7, 7, 6, 2, 7, 9, 1, 1, 2, 6, 7, 7, 7, 1, 1, 1, 4, 5, 7, 7, 7, 2, 5, 7, 1, 6, 7, 1, 5, 5, 6, 7, 2, 1, 2, 4, 2, 7, 7, 4, 1, 5, 7, 2, 5, 6, 4, 5, 7, 5, 7, 7, 1, 7, 7, 5, 7, 3, 2, 1, 7, 6, 7, 7, 4, 1, 1, 1, 1, 1, 5, 1, 7, 1, 1, 1, 1, 2, 4, 1, 4, 4, 7, 2, 1, 1, 7, 2, 1, 1, 6, 7, 1, 6, 7, 4, 7, 7, 7, 4, 1, 7, 1, 1, 7, 1, 4, 7, 5, 2, 7, 7, 2, 7, 7, 1, 7, 4, 2, 4, 4, 6, 4, 1]\n",
      "368\n",
      "368\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Kaggle competition test data set for target class labels are incomplete and as such our test prediction \n",
    "    and true values need to be reconciled appropriately\n",
    "\"\"\"\n",
    "y_true_data = pd.read_csv('stage1_solution_filtered.csv')\n",
    "\n",
    "y_final_pred = [y_pred[d] for d in y_true_data['ID']]\n",
    "model_predictions_final = [model_predictions[d] for d in y_true_data['ID']]\n",
    "\n",
    "y_final_pred_labels = [ np.argmax(pred) + 1 for pred in y_final_pred]\n",
    "print y_final_pred_labels[0]\n",
    "\n",
    "y_true_data.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "y_true_data_labels = [np.argmax(data) + 1 for data in np.array(y_true_data)]\n",
    "print y_true_data_labels\n",
    "\n",
    "print len(y_final_pred_labels)\n",
    "print len(y_true_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.81731040711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "#print model_predictions_final\n",
    "\n",
    "print log_loss(y_true_data_labels, model_predictions_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
