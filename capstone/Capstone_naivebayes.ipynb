{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  Class\n",
       "0   0  FAM58A  Truncating Mutations      1\n",
       "1   1     CBL                 W802*      2\n",
       "2   2     CBL                 Q249E      2\n",
       "3   3     CBL                 N454D      3\n",
       "4   4     CBL                 L399V      4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_variant_data = pd.read_csv('training_variants')\n",
    "X_train_variant_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321\n"
     ]
    }
   ],
   "source": [
    "print X_train_variant_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ACSL4</td>\n",
       "      <td>R570S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NAGLU</td>\n",
       "      <td>P521L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PAH</td>\n",
       "      <td>L333F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ING1</td>\n",
       "      <td>A148D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>TMEM216</td>\n",
       "      <td>G77A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Gene Variation\n",
       "0   0    ACSL4     R570S\n",
       "1   1    NAGLU     P521L\n",
       "2   2      PAH     L333F\n",
       "3   3     ING1     A148D\n",
       "4   4  TMEM216      G77A"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_variant_data = pd.read_csv('test_variants')\n",
    "X_test_variant_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2. This mutation resulted in a myeloproliferat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Abstract The Large Tumor Suppressor 1 (LATS1)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Vascular endothelial growth factor receptor (V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Inflammatory myofibroblastic tumor (IMT) is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Abstract Retinoblastoma is a pediatric retina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Text\n",
       "0   0  2. This mutation resulted in a myeloproliferat...\n",
       "1   1   Abstract The Large Tumor Suppressor 1 (LATS1)...\n",
       "2   2  Vascular endothelial growth factor receptor (V...\n",
       "3   3  Inflammatory myofibroblastic tumor (IMT) is a ...\n",
       "4   4   Abstract Retinoblastoma is a pediatric retina..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text = pd.read_table('training_text', sep='\\|\\|', engine='python', names=['ID', 'Text'], skiprows=[0])\n",
    "X_test_text = pd.read_table('test_text', sep='\\|\\|', engine='python', names=['ID', 'Text'], skiprows=[0])\n",
    "X_test_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2996\n"
     ]
    }
   ],
   "source": [
    "print len(np.unique(X_train_variant_data['Variation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    2\n",
       "3    3\n",
       "4    4\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Remove the class target label from the training variant data set and add it to an independent target label array\n",
    "\"\"\"\n",
    "y_train = X_train_variant_data['Class']\n",
    "X_train_variant_data.drop('Class', axis=1, inplace=True)\n",
    "y_train.head()\n",
    "#print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>class4</th>\n",
       "      <th>class5</th>\n",
       "      <th>class6</th>\n",
       "      <th>class7</th>\n",
       "      <th>class8</th>\n",
       "      <th>class9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class1  class2  class3  class4  class5  class6  class7  class8  class9\n",
       "0       1       0       0       0       0       0       0       0       0\n",
       "1       0       1       0       0       0       0       0       0       0\n",
       "2       0       1       0       0       0       0       0       0       0\n",
       "3       0       0       1       0       0       0       0       0       0\n",
       "4       0       0       0       1       0       0       0       0       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    One hot encode the target label array\n",
    "\"\"\"\n",
    "\n",
    "y_train = pd.get_dummies(y_train, prefix='class', prefix_sep='')\n",
    "y_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 500)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Hash vectorize gene, variation columns\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "\n",
    "X_train_gene = X_train_variant_data['Gene']\n",
    "gene_hash_vectorizer = HashingVectorizer(n_features=500)\n",
    "gene_vector = gene_hash_vectorizer.transform(X_train_gene)\n",
    "\n",
    "X_train_variation = X_train_variant_data['Variation']\n",
    "variation_hash_vectorizer = HashingVectorizer(n_features=5000)\n",
    "variation_vector = variation_hash_vectorizer.transform(X_train_variation)\n",
    "\n",
    "X_test_gene = X_test_variant_data['Gene']\n",
    "gene_test_hash_vectorizer = HashingVectorizer(n_features=500)\n",
    "gene_test_vector = gene_test_hash_vectorizer.transform(X_test_gene)\n",
    "\n",
    "X_test_variation = X_test_variant_data['Variation']\n",
    "variation_test_hash_vectorizer = HashingVectorizer(n_features=5000)\n",
    "variation_test_vector = variation_test_hash_vectorizer.transform(X_test_variation)\n",
    "\n",
    "print gene_vector.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/paperspace/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Download nltk stopwords corpus\n",
    "\"\"\"\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'opinion', u'guarante', u'life']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Process clinical text data by tokenizing words, removing stop words, stemming words etc.\n",
    "\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import utils\n",
    "def process_clinical_text(clinical_text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenized_text = word_tokenize(utils.to_unicode(clinical_text))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    processed_text = []\n",
    "    for word in tokenized_text:\n",
    "        if word not in stop_words:\n",
    "            processed_text.append(stemmer.stem(word))\n",
    "    return processed_text\n",
    "\n",
    "print process_clinical_text('I am of the opinion that there is no guarantee any where in life')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Apply CountVectorizer model on the clinical text training and test data\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "processed_clinical_text = X_train_text['Text']\n",
    "count_vector.fit(processed_clinical_text)\n",
    "clinical_text_train = count_vector.transform(processed_clinical_text).toarray()\n",
    "processed_clinical_test_text = X_test_text['Text']\n",
    "clinical_text_test = count_vector.transform(processed_clinical_test_text).toarray()\n",
    "print clinical_text_test[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 155732)\n",
      "(5668, 155732)\n"
     ]
    }
   ],
   "source": [
    "print clinical_text_train.shape\n",
    "print clinical_text_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 161232)\n",
      "(5668, 161232)\n",
      "(3321, 9)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Combine gene vector, variation vector and clinical text vector into a resultant training set\n",
    "\"\"\"\n",
    "\n",
    "X_train = np.hstack((gene_vector.toarray(), variation_vector.toarray(), clinical_text_train))\n",
    "X_test = np.hstack((gene_test_vector.toarray(), variation_test_vector.toarray(), clinical_text_test))\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5668, 161232, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Expand dimensions of data to fit into the CNN model\n",
    "\"\"\"\n",
    "\n",
    "X_train_dim_data = np.expand_dims(X_train, axis=2)\n",
    "#print X_train_dim_data.shape\n",
    "y_train_dim_data = np.array(y_train)\n",
    "#print y_train_dim_data.shape\n",
    "\n",
    "X_test_dim_data = np.expand_dims(X_test, axis=2)\n",
    "print X_test_dim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Define and compile the CNN model\n",
    "\"\"\"\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "def create_cnn_model(optimizer='rmsprop'):\n",
    "    num_classes = 9\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=X_train_dim_data.shape[1:]))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_model.add(Dropout(0.2))\n",
    "    cnn_model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_model.add(Dropout(0.2))\n",
    "    cnn_model.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_model.add(Dropout(0.2))\n",
    "    cnn_model.add(GlobalAveragePooling1D())\n",
    "    cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    cnn_model.summary()\n",
    "    cnn_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 9)\n",
      "[1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Reshape the training inputs for KFold cross validation inputs\n",
    "\"\"\"\n",
    "X_train_2d = X_train_dim_data.reshape(X_train_dim_data.shape[0], X_train_dim_data.shape[1])\n",
    "y_train_2d = y_train_dim_data.reshape(y_train_dim_data.shape[0], y_train_dim_data.shape[1])\n",
    "print y_train_2d.shape\n",
    "print y_train_2d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 161232, 16)        48        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 80616, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 80616, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 80616, 32)         1056      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 40308, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40308, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 40308, 64)         4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 20154, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20154, 64)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 5,849\n",
      "Trainable params: 5,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2988 samples, validate on 333 samples\n",
      "Epoch 1/3\n",
      "2980/2988 [============================>.] - ETA: 0s - loss: 1.8713 - acc: 0.2832Epoch 00000: val_loss improved from inf to 1.89023, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2988/2988 [==============================] - 75s - loss: 1.8709 - acc: 0.2838 - val_loss: 1.8902 - val_acc: 0.3303\n",
      "Epoch 2/3\n",
      "2980/2988 [============================>.] - ETA: 0s - loss: 1.8278 - acc: 0.2782Epoch 00001: val_loss improved from 1.89023 to 1.86886, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2988/2988 [==============================] - 73s - loss: 1.8274 - acc: 0.2784 - val_loss: 1.8689 - val_acc: 0.3303\n",
      "Epoch 3/3\n",
      "2980/2988 [============================>.] - ETA: 0s - loss: 1.8257 - acc: 0.2809Epoch 00002: val_loss improved from 1.86886 to 1.85049, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2988/2988 [==============================] - 73s - loss: 1.8266 - acc: 0.2805 - val_loss: 1.8505 - val_acc: 0.3303\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8316 - acc: 0.3144Epoch 00000: val_loss improved from 1.85049 to 1.81959, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 73s - loss: 1.8305 - acc: 0.3158 - val_loss: 1.8196 - val_acc: 0.0271\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8274 - acc: 0.3161Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8273 - acc: 0.3158 - val_loss: 1.8434 - val_acc: 0.0271\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8273 - acc: 0.3164Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8274 - acc: 0.3158 - val_loss: 1.8381 - val_acc: 0.0271\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8342 - acc: 0.2785Epoch 00000: val_loss improved from 1.81959 to 1.78126, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 73s - loss: 1.8346 - acc: 0.2794 - val_loss: 1.7813 - val_acc: 0.3524\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8298 - acc: 0.2752Epoch 00001: val_loss improved from 1.78126 to 1.76961, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 73s - loss: 1.8299 - acc: 0.2757 - val_loss: 1.7696 - val_acc: 0.3524\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8293 - acc: 0.2772Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8295 - acc: 0.2774 - val_loss: 1.7881 - val_acc: 0.3614\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8259 - acc: 0.2923Epoch 00000: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8249 - acc: 0.2931 - val_loss: 1.8086 - val_acc: 0.2620\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8251 - acc: 0.2889Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8238 - acc: 0.2897 - val_loss: 1.8192 - val_acc: 0.2620\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8232 - acc: 0.2906Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8239 - acc: 0.2904 - val_loss: 1.8043 - val_acc: 0.2620\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8266 - acc: 0.2681Epoch 00000: val_loss improved from 1.76961 to 1.75919, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 73s - loss: 1.8269 - acc: 0.2683 - val_loss: 1.7592 - val_acc: 0.4458\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8256 - acc: 0.2654Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8259 - acc: 0.2656 - val_loss: 1.7826 - val_acc: 0.4036\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8251 - acc: 0.2705Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8246 - acc: 0.2713 - val_loss: 1.7695 - val_acc: 0.4367\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8156 - acc: 0.2691Epoch 00000: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8161 - acc: 0.2693 - val_loss: 1.8401 - val_acc: 0.3645\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8139 - acc: 0.2685Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8144 - acc: 0.2687 - val_loss: 1.8392 - val_acc: 0.3584\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8163 - acc: 0.2815Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8156 - acc: 0.2824 - val_loss: 1.8566 - val_acc: 0.3313\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8275 - acc: 0.2936Epoch 00000: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8282 - acc: 0.2927 - val_loss: 1.7686 - val_acc: 0.2410\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8265 - acc: 0.2916Epoch 00001: val_loss improved from 1.75919 to 1.74150, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 73s - loss: 1.8259 - acc: 0.2917 - val_loss: 1.7415 - val_acc: 0.2410\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8238 - acc: 0.2919Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8244 - acc: 0.2921 - val_loss: 1.7582 - val_acc: 0.2410\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.7802 - acc: 0.3087Epoch 00000: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.7794 - acc: 0.3091 - val_loss: 2.2651 - val_acc: 0.0964\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.7764 - acc: 0.3107Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.7759 - acc: 0.3105 - val_loss: 2.2584 - val_acc: 0.0994\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.7742 - acc: 0.3040Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.7743 - acc: 0.3044 - val_loss: 2.2134 - val_acc: 0.0964\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.7975 - acc: 0.2805Epoch 00000: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.7980 - acc: 0.2800 - val_loss: 2.0445 - val_acc: 0.1928\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.7975 - acc: 0.2842Epoch 00001: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.7966 - acc: 0.2844 - val_loss: 2.0422 - val_acc: 0.2199\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.7969 - acc: 0.2826Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.7961 - acc: 0.2834 - val_loss: 2.0461 - val_acc: 0.2108\n",
      "Train on 2989 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8336 - acc: 0.2688Epoch 00000: val_loss improved from 1.74150 to 1.69772, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 73s - loss: 1.8335 - acc: 0.2683 - val_loss: 1.6977 - val_acc: 0.3163\n",
      "Epoch 2/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8309 - acc: 0.2701Epoch 00001: val_loss improved from 1.69772 to 1.68262, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2989/2989 [==============================] - 73s - loss: 1.8321 - acc: 0.2697 - val_loss: 1.6826 - val_acc: 0.3584\n",
      "Epoch 3/3\n",
      "2980/2989 [============================>.] - ETA: 0s - loss: 1.8309 - acc: 0.2758Epoch 00002: val_loss did not improve\n",
      "2989/2989 [==============================] - 73s - loss: 1.8317 - acc: 0.2750 - val_loss: 1.6968 - val_acc: 0.3434\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Train the CNN model\n",
    "\"\"\"\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "cv_fold = KFold(n_splits=10)\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "for train_index, test_index in cv_fold.split(X_train_2d):\n",
    "    X_fold_train, X_fold_test = X_train_2d[train_index], X_train_2d[test_index]\n",
    "    y_fold_train, y_fold_test = y_train_2d[train_index], y_train_2d[test_index]\n",
    "    X_fold_train = np.expand_dims(X_fold_train, axis=2)\n",
    "    X_fold_test = np.expand_dims(X_fold_test, axis=2)    \n",
    "    cnn_model.fit(X_fold_train, y_fold_train, validation_data=(X_fold_test, y_fold_test), epochs=3, batch_size=20, \n",
    "              callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Load the best weights into the model\n",
    "\"\"\"\n",
    "cnn_model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18649754  0.12723508  0.02971044  0.21590438  0.08643806  0.08711641\n",
      "  0.24601415  0.00724449  0.01383942]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Make predictions from test data\n",
    "\"\"\"\n",
    "\n",
    "model_predictions = cnn_model.predict(X_test_dim_data)\n",
    "\n",
    "print model_predictions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(len(model_predictions)):\n",
    "    max_value = model_predictions[i][np.argmax(model_predictions[i])]\n",
    "    y_pred.append(model_predictions[i] // max_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print y_pred[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18592404  0.12709844  0.02955492  0.21704428  0.08652719  0.08736788\n",
      "  0.24605358  0.00699793  0.01343174]\n",
      "[[1 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]]\n",
      "[1, 2, 2, 4, 4, 4, 9, 7, 7, 7, 2, 1, 4, 1, 1, 1, 2, 3, 2, 7, 2, 1, 7, 7, 3, 7, 9, 7, 1, 4, 1, 6, 4, 9, 6, 4, 1, 5, 2, 1, 3, 2, 7, 7, 5, 7, 4, 6, 4, 1, 1, 4, 7, 1, 6, 7, 1, 1, 7, 2, 7, 2, 1, 7, 4, 9, 7, 4, 4, 1, 4, 5, 1, 1, 1, 4, 2, 2, 1, 7, 1, 5, 6, 7, 6, 6, 1, 7, 5, 2, 1, 4, 7, 7, 4, 1, 1, 4, 7, 1, 1, 2, 3, 7, 2, 2, 7, 4, 1, 4, 4, 7, 4, 7, 7, 7, 6, 3, 7, 7, 4, 4, 4, 2, 4, 1, 4, 1, 4, 1, 2, 4, 7, 7, 4, 7, 7, 7, 4, 1, 2, 7, 4, 1, 1, 7, 2, 7, 7, 7, 2, 2, 1, 4, 1, 1, 7, 7, 1, 5, 4, 4, 7, 2, 1, 2, 4, 6, 4, 7, 1, 1, 1, 1, 7, 7, 4, 5, 4, 1, 4, 2, 6, 7, 4, 5, 7, 3, 1, 7, 6, 2, 1, 8, 7, 7, 5, 7, 5, 5, 1, 1, 1, 7, 7, 2, 1, 1, 7, 1, 2, 1, 4, 9, 1, 2, 7, 7, 1, 2, 4, 1, 4, 7, 7, 1, 4, 8, 4, 5, 4, 4, 5, 1, 4, 1, 2, 7, 2, 4, 1, 6, 6, 5, 7, 7, 6, 2, 7, 9, 1, 1, 2, 6, 7, 7, 7, 1, 1, 1, 4, 5, 7, 7, 7, 2, 5, 7, 1, 6, 7, 1, 5, 5, 6, 7, 2, 1, 2, 4, 2, 7, 7, 4, 1, 5, 7, 2, 5, 6, 4, 5, 7, 5, 7, 7, 1, 7, 7, 5, 7, 3, 2, 1, 7, 6, 7, 7, 4, 1, 1, 1, 1, 1, 5, 1, 7, 1, 1, 1, 1, 2, 4, 1, 4, 4, 7, 2, 1, 1, 7, 2, 1, 1, 6, 7, 1, 6, 7, 4, 7, 7, 7, 4, 1, 7, 1, 1, 7, 1, 4, 7, 5, 2, 7, 7, 2, 7, 7, 1, 7, 4, 2, 4, 4, 6, 4, 1]\n",
      "368\n",
      "368\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Kaggle competition test data set for target class labels are incomplete and as such our test prediction \n",
    "    and true values need to be reconciled appropriately\n",
    "\"\"\"\n",
    "y_true_data = pd.read_csv('stage1_solution_filtered.csv')\n",
    "#print y_true_data['ID']\n",
    "y_final_pred = [y_pred[d] for d in y_true_data['ID']]\n",
    "model_predictions_final = [model_predictions[d] for d in y_true_data['ID']]\n",
    "print model_predictions_final[0]\n",
    "y_final_pred_labels = [ np.argmax(pred) + 1 for pred in y_final_pred]\n",
    "#print y_final_pred_labels[0]\n",
    "\n",
    "y_true_data.drop('ID', axis=1, inplace=True)\n",
    "print np.array(y_true_data)\n",
    "y_true_data_labels = [np.argmax(data) + 1 for data in np.array(y_true_data)]\n",
    "print y_true_data_labels\n",
    "\n",
    "print len(y_final_pred_labels)\n",
    "print len(y_true_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368, 9)\n",
      "(368, 9)\n",
      "1.78656935668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "#print model_predictions_final\n",
    "\n",
    "y_true_data_final = np.array(y_true_data)\n",
    "print y_true_data_final.shape\n",
    "print np.array(model_predictions_final).shape\n",
    "#print log_loss(y_true_data_labels, model_predictions_final)\n",
    "print log_loss(y_true_data_final, np.array(model_predictions_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
